

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Introduction &mdash; DFDistill  documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=fd3f3429" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=5929fcd5"></script>
      <script src="_static/doctools.js?v=9a2dae69"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Installation" href="installation.html" />
    <link rel="prev" title="Data-Free Distillation" href="info.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            DFDistill
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Main Info:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="info.html">Data-Free Distillation</a></li>
<li class="toctree-l1"><a class="reference internal" href="info.html#description">Description</a></li>
<li class="toctree-l1"><a class="reference internal" href="info.html#algorithms-implemented">Algorithms Implemented</a></li>
<li class="toctree-l1"><a class="reference internal" href="info.html#related-work">Related Work</a></li>
<li class="toctree-l1"><a class="reference internal" href="info.html#tech-stack">Tech Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="info.html#links">Links</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Theoretical Background:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="#theoretical-background">Theoretical Background</a></li>
<li class="toctree-l1"><a class="reference internal" href="#dkdf-algorithms">DKDF Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="#our-framework">Our Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Get Started:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">DFDistill</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Introduction</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/Intelligent-Systems-Phystech/DFDistill/blob/main./docs/blogpost.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h1>
<p>Knowledge distillation (KD) is a powerful technique for transferring knowledge from a large, complex “teacher” model to a smaller, more efficient “student” model.
Traditionally, this process relies on access to the original training data. However, in many real-world scenarios, such as when dealing with sensitive information
or legacy systems, the original data may be unavailable or inaccessible. Data-free distillation addresses this challenge by synthesizing “surrogate” data directly
from the teacher model, enabling effective knowledge transfer even in the absence of the original dataset.</p>
<p><strong>Why Data-Free Distillation?</strong></p>
<ul class="simple">
<li><p><em>Data Privacy:</em> Medical, financial, or user-generated data often cannot be shared due to legal or ethical constraints.</p></li>
<li><p><em>Legacy Systems:</em> Original training data might be lost or corrupted over time.</p></li>
<li><p><em>Resource Efficiency:</em> Generating synthetic data tailored to the teacher’s expertise avoids costly data collection.</p></li>
</ul>
<p>The core idea is to reverse-engineer representative samples from the teacher model’s internal representations (e.g., feature statistics, spectral patterns, or
adversarial examples) and use these to train the student.</p>
</section>
<section id="theoretical-background">
<h1>Theoretical Background<a class="headerlink" href="#theoretical-background" title="Link to this heading"></a></h1>
<p>Let <span class="math notranslate nohighlight">\(T\)</span> be a pre-trained teacher model with parameters <span class="math notranslate nohighlight">\(\theta_T\)</span>, and <span class="math notranslate nohighlight">\(S\)</span> a student model with parameters <span class="math notranslate nohighlight">\(\theta_S\)</span>. Traditional KD minimizes the Kullback-Leibler
(KL) divergence between the teacher’s and student’s output distributions over a dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \{x_i\}_{i=1}^N\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{KD}} = \mathbb{E}_{x \sim \mathcal{D}} \left[ \text{KL}\left(T(x) \, \| \, S(x)\right) \right]\]</div>
<p>Here, <span class="math notranslate nohighlight">\(T(x)\)</span> and <span class="math notranslate nohighlight">\(S(x)\)</span> are probability distributions of the teacher and student, respectively. This approach requires access to
<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, which becomes problematic when data is unavailable.</p>
<a class="reference internal image-reference" href="_images/ClassicalDistill.png"><img alt="The workflow of classical KD" class="align-center" src="_images/ClassicalDistill.png" style="width: 65%;" />
</a>
<p><strong>Transition to Data-Free Knowledge Distillation (DFKD)</strong></p>
<p>In our case, when <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is inaccessible, DFKD synthesizes a surrogate dataset <span class="math notranslate nohighlight">\(\mathcal{D}' = \{x'_i\}_{i=1}^M\)</span> directly from <span class="math notranslate nohighlight">\(T\)</span>. Therefore,
the objective generalizes to:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{DFKD}} = \mathbb{E}_{x' \sim \mathcal{D}'} \left[ \text{KL}\left(T(x') \, \| \, S(x')\right) \right]\]</div>
<p>The key challenge here lies in designing high quality surrogate dataset <span class="math notranslate nohighlight">\(\mathcal{D}'\)</span> that is able to capture the inherent knowledge of <span class="math notranslate nohighlight">\(T\)</span>.</p>
<a class="reference internal image-reference" href="_images/DFDistill_Framework.png"><img alt="The workflow of DFKD" class="align-center" src="_images/DFDistill_Framework.png" style="width: 95%;" />
</a>
</section>
<section id="dkdf-algorithms">
<h1>DKDF Algorithms<a class="headerlink" href="#dkdf-algorithms" title="Link to this heading"></a></h1>
<p>In our work, we investigate several DKDF algorithms, which are briefly described below:
- <strong>Statistical Feature Matching</strong>
Let <span class="math notranslate nohighlight">\(\phi_T^l(x)\)</span> and <span class="math notranslate nohighlight">\(\phi_S^l(x)\)</span> denote the activations at layer <span class="math notranslate nohighlight">\(l\)</span> of <span class="math notranslate nohighlight">\(T\)</span> and <span class="math notranslate nohighlight">\(S\)</span>. We synthesize <span class="math notranslate nohighlight">\(\mathcal{D}'\)</span> by aligning
batch normalization statistics:</p>
<div class="math notranslate nohighlight">
\[\min_{x'} \sum_{l} \left( \|\mu_T^l - \mu_S^l(x')\|_2^2 + \|\sigma_T^l - \sigma_S^l(x')\|_2^2 \right),\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu_T^l, \sigma_T^l\)</span> are the teacher’s precomputed mean and standard deviation.</p>
<a class="reference internal image-reference" href="_images/DFDistill_Framework.png"><img alt="The workflow of Statistical Feature Matching" class="align-center" src="_images/DFDistill_Framework.png" style="width: 60%;" />
</a>
<ul class="simple">
<li><p><strong>Spectral Feature Matching</strong></p></li>
<li><p><strong>Adversarial Distillation</strong></p></li>
<li><p><strong>Deep Inversion</strong></p></li>
</ul>
</section>
<section id="our-framework">
<h1>Our Framework<a class="headerlink" href="#our-framework" title="Link to this heading"></a></h1>
<p>We now present the unified framework that allows one to use various specific DFKD algorithms to distill the knowledge from any source without having access
to the initial data. Our implementation is a library that allows, given a teacher model (and possibly some statistics), to train a distilled student model
out-of-the-box. The user can utilize any of the above-described algorithms as well as the classical KD approach if the original data is given.</p>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="info.html" class="btn btn-neutral float-left" title="Data-Free Distillation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="installation.html" class="btn btn-neutral float-right" title="Installation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Nasyrov E., Okhotnikov N., Sapronov Y., Solodkin V..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>